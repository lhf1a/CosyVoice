
import os
import sys
print(f"DEBUG SYSTARGV: {sys.argv}")
import argparse
import logging
import time
import numpy as np
from typing import Optional, Generator
from concurrent.futures import ThreadPoolExecutor
import threading
import torch
import torchaudio
from datetime import datetime
from fastapi.responses import FileResponse

# æ–¹æ³•1.1ï¼šä½¿ç”¨æ—¶é—´æˆ³+éšæœºæ•°
import uuid

def generate_filename_with_timestamp(prefix="", suffix="", extension="txt"):
    """ç”ŸæˆåŸºäºæ—¶é—´æˆ³çš„å”¯ä¸€æ–‡ä»¶å"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_id = str(uuid.uuid4())[:8]  # å–UUIDå‰8ä½
    
    # ç¡®ä¿ temp_output ç›®å½•å­˜åœ¨
    output_dir = os.path.join(SCRIPT_DIR, '../temp_output')
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    filename = os.path.join(output_dir, f"{prefix}{timestamp}_{unique_id}{suffix}.{extension}")
    return filename

# è®¾ç½®è·¯å¾„
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, SCRIPT_DIR)
sys.path.insert(0, os.path.join(SCRIPT_DIR, '..'))
sys.path.insert(0, os.path.join(SCRIPT_DIR, '../third_party', 'Matcha-TTS'))

from fastapi import FastAPI, Form, File, UploadFile, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ============================================================================
# å¤šéŸ³è‰²é…ç½® (ç”¨æˆ·å¯ä¿®æ”¹æ­¤éƒ¨åˆ†)
# ============================================================================
# æ ¼å¼: {"id": "éŸ³è‰²ID", "file": "æ–‡ä»¶å", "prompt_text": "éŸ³é¢‘ä¸­è¯´çš„è¯"}
# æ–‡ä»¶æ”¾åœ¨ deploy/cosyvoice/asset/ ç›®å½•ä¸‹
# è¦æ±‚éŸ³é¢‘æ¸…æ™°ï¼ˆ5~15ç§’ä¸ºä½³ï¼Œä¸è¦è¿‡é•¿ï¼‰ï¼Œä¿å­˜ä¸ºwavæ ¼å¼ï¼Œé‡‡é›†ç‡16kHz,å•å£°é“ï¼Œå†…å®¹ä»»æ„ï¼Œä½†æ˜¯å’Œprompt_texté‡Œçš„å†…å®¹å®Œå…¨ä¸€è‡´ï¼Œå¿…é¡»ä¸€å­—ä¸å·®ï¼
# CosyVoice3 çš„ prompt_text å¿…é¡»ä»¥ "You are a helpful assistant.<|endofprompt|>" å¼€å¤´
# ============================================================================
VOICE_CONFIGS = [
    {
        "id": "default",  # é»˜è®¤éŸ³è‰²
        "file": "zero_shot_prompt.wav",  # asset/zero_shot_prompt.wav
        "prompt_text": "You are a helpful assistant.<|endofprompt|>å¸Œæœ›ä½ ä»¥åèƒ½å¤Ÿåšçš„æ¯”æˆ‘è¿˜å¥½å‘¦ã€‚",
        "type":"female"
    },
    # æ·»åŠ æ›´å¤šéŸ³è‰²ç¤ºä¾‹ (å–æ¶ˆæ³¨é‡Šå¹¶ä¿®æ”¹):
    {
        "id": "longyingcheng",
        "file": "longyingcheng_man.wav",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>çœŸä¸å¥½æ„æ€ï¼Œä»å°è‡³ä»Šï¼Œä»–è¿˜ä»æ¥æ²¡æœ‰è¢«å“ªä¸€ä½å¼‚æ€§æœ‹å‹äº²å»è¿‡å‘¢ã€‚",
        "type":"male"
    },
    {
        "id": "longyingwan",
        "file": "longyingwan_woman.wav", 
        "prompt_text": "You are a helpful assistant.<|endofprompt|>æˆ‘ä»¬å°†ä¸ºå…¨çƒåŸå¸‚çš„å¯æŒç»­å‘å±•è´¡çŒ®åŠ›é‡ã€‚",
        "type":"female"
    },
    {
        "id": "longyingmu",
        "file": "longyingmu_woman.wav",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>æ‚¨å¥½ï¼Œæˆ‘æ˜¯æ™ºèƒ½ç”µè¯åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æ‚¨éœ€è¦å’¨è¯¢ä¸šåŠ¡é¢„çº¦åŠç†è¿˜æ˜¯æŸ¥è¯¢ä¿¡æ¯ï¼Ÿ",
        "type":"female"
    
    },
    {
        "id": "nanami",
        "file": "nanami.wav",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>ä¸‰ç™¾å¤šä¸‡ä¹Ÿä¸ä¾¿å®œï¼Œå“å‘€ä½ è¦æƒ³ä¸Šæµ·çš„æˆ¿ä»·ï¼Œä½ æƒ³ä¸Šæµ·å¹³å‡éƒ½æ˜¯å¤šå°‘ã€‚",
        "type":"female"
    
    },
    {
        "id": "female_sunny",
        "file": "female_sunny.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>å¿«æ¥,å¸®æˆ‘çœ‹çœ‹ä»Šå¤©ä¸‹åˆèŒ¶å–ä»€ä¹ˆ?"
    },
    {
        "id": "female_hi",
        "file": "female_hi.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>æ—¥å…‰æœ‰äº›åˆºçœ¼,è¿™é¡¶å¸½å­å°±å€Ÿç»™ä½ å§!æ”¾å¿ƒ,ä¸ä¼šæœ‰é¸½å­é£å‡ºæ¥çš„ã€‚"
    },
    {
        "id": "female_lazy",
        "file": "female_lazy.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>ç¡ä¸ç€çš„è¯,å°±ä¸€èµ·ç©ä¸ªé€šå®µ!"
    },
    {
        "id": "female_middle",
        "file": "female_middle.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>åƒè¿‡åˆé¥­äº†å—?æ²¡æœ‰çš„è¯,è¦ä¸è¦å’Œæˆ‘ä¸€èµ·?"
    },
    {
        "id": "female_maid",
        "file": "female_maid.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>æ—©å®‰,é­”ç‹å¤§äºº,è¯·é—®æ‚¨éœ€è¦èŒ¶è¿˜æ˜¯å’–å•¡......"
    },
    {
        "id": "female_loli",
        "file": "female_loli.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>ä»Šå¤©çš„åˆå¸‚ç”Ÿæ„ç‰¹åˆ«å¥½,æˆ‘ä¹Ÿè¦åŠ æ²¹å’¯!"
    },
    {
        "id": "female_reader",
        "file": "female_reader.wav",
        "type":"female",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>å®è´ä»¬ï¼Œè¿™å‡ å¤©æˆ‘ç‰¹åˆ«æƒ³å‘Šè¯‰ä½ ä¸€ä¸ªå¤©å¤§çš„ç§˜å¯†ï¼Œå…³ç³»åˆ°ä½ ä¸‹åŠå¹´çš„è´¢è¿ã€‚è®°ä½2æœˆ31å·è¿™å¤©ï¼Œä¸€å®šè¦æ¥æ‰¾æˆ‘ï¼Œæˆ‘æ‚„æ‚„å‘Šè¯‰ä½ ã€‚ç°åœ¨èµ¶ç´§ç‚¹èµæ”¶è—ï¼Œè®©æˆ‘çŸ¥é“ä½ åœ¨ï¼Œåˆ«è®©å¥½è¿ä»èº«è¾¹æºœèµ°å•Šï¼"
    },
    {
        "id": "male_low",
        "file": "male_low.wav",
        "type":"male",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>äººä¸–é—´æœ‰å¤ªå¤šæ— å¥ˆï¼Œå¦‚æƒ³å»å‘¨æ¸¸ä¸–ç•Œã€ä½†åˆæ²¡æœ‰é’±ã€‚è¨€åˆ°æ­¤å¤„ï¼Œæˆ‘æƒ³èµ·ã€Šé‡‘ç“¶æ¢…ã€‹ä¸­çš„é‚£å¥è¯ï¼Œå‡¡äº‹éƒ½è¦å¬å¬è±†åŒ…çš„ï¼Œå”¯æœ‰è±†åŒ…ï¼Œæ˜¯è®¤çœŸå¬æˆ‘ä»¬è®²è¯çš„ã€‚"
    },
    {
        "id": "midmale",
        "file": "midmale.wav",
        "prompt_text": "You are a helpful assistant.<|endofprompt|>è‡³äºå¥¹çš„ç”Ÿæ´»æ¥æºï¼Œæ˜¯åœ¨ç½‘ç»œä¸Šå†™ä¸€äº›å°çŸ­æ–‡ï¼Œèµšäº›é’±ã€‚",
        "type":"male"
    
    }
]
# ============================================================================

# å…¨å±€å˜é‡
cosyvoice = None
inference_lock = threading.Lock()

# å¤šéŸ³è‰²ç¼“å­˜: {voice_id: {"file": path, "prompt_text": text}}
voice_cache = {}
default_voice_id = "default"  # é»˜è®¤ä½¿ç”¨çš„éŸ³è‰²ID

# è¾“å‡ºé‡‡æ ·ç‡ (å¯é€šè¿‡ --output_sample_rate é…ç½®)
# é»˜è®¤ 16000 ä»¥å…¼å®¹å°æ™ºå¹³å°
output_sample_rate = 24000

# [æ³¨æ„] CosyVoice API ä¸æ”¯æŒä¼ å…¥ Tensorï¼Œå¿…é¡»ä¼ è·¯å¾„æˆ–æ–‡ä»¶å¯¹è±¡ï¼Œå› æ­¤ç§»é™¤ resampler_cache ä¼˜åŒ–


app = FastAPI(title="CosyVoice TTS Server", version="1.0.0")

# CORS é…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"]
)

def generate_instruct2(
    text: str,
    voice_id: str,
    motionctrl: str = None,
):
    global cosyvoice, voice_cache
    logger.info(f"å‚æ•°: voice_id{voice_id} {text} {motionctrl}")
    # ç¡®å®šä½¿ç”¨å“ªä¸ªéŸ³è‰²
    wavfile =  generate_filename_with_timestamp(prefix=voice_id, extension="wav")
    if voice_id not in voice_cache:
        raise ValueError(f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    spk_id = voice_id
    voice_info = voice_cache[voice_id]
    actual_prompt_text = motionctrl
    actual_prompt_wav = voice_info["file"]
    for i, j in enumerate(cosyvoice.inference_instruct2(text,actual_prompt_text,actual_prompt_wav, stream=False)):
       torchaudio.save(wavfile.format(i), j['tts_speech'], cosyvoice.sample_rate)
    return wavfile


def generate_zs_sync(
    text: str,
    voice_id: str,
):
    global cosyvoice, voice_cache
    logger.info(f"å‚æ•°: voice_id{voice_id} {text}")
    # ç¡®å®šä½¿ç”¨å“ªä¸ªéŸ³è‰²
    wavfile =  generate_filename_with_timestamp(prefix=voice_id, extension="wav")
    if voice_id not in voice_cache:
        raise ValueError(f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    voice_info = voice_cache[voice_id]
    actual_prompt_text = voice_info["prompt_text"]
    actual_prompt_wav = voice_info["file"]
    for i, j in enumerate(cosyvoice.inference_zero_shot(text,actual_prompt_text,actual_prompt_wav, stream=False)):
       torchaudio.save(wavfile.format(i), j['tts_speech'], cosyvoice.sample_rate)
    return wavfile

def generate_cross_lingual_sync(
    text: str,
    voice_id: str,
):
    global cosyvoice, voice_cache
    logger.info(f"å‚æ•°: voice_id{voice_id} {text}")
    # ç¡®å®šä½¿ç”¨å“ªä¸ªéŸ³è‰²
    wavfile =  generate_filename_with_timestamp(prefix=voice_id, extension="wav")
    if voice_id not in voice_cache:
        raise ValueError(f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    voice_info = voice_cache[voice_id]
    actual_prompt_text = voice_info["prompt_text"]
    actual_prompt_wav = voice_info["file"]
    for i, j in enumerate(cosyvoice.inference_cross_lingual(f"You are a helpful assistant.<|endofprompt|>{text}",actual_prompt_wav, stream=False)):
       torchaudio.save(wavfile.format(i), j['tts_speech'], cosyvoice.sample_rate)
    return wavfile




def generate_audio_stream(
    text: str,
    voice_id: str = None,
    prompt_text: str = None,
    prompt_wav = None,
    stream: bool = True
) -> Generator[bytes, None, None]:
    """
    ç”Ÿæˆæµå¼éŸ³é¢‘æ•°æ®
    
    Args:
        text: è¦åˆæˆçš„æ–‡æœ¬
        voice_id: éŸ³è‰²ID (ä½¿ç”¨é¢„åŠ è½½çš„ç¼“å­˜éŸ³è‰²ï¼Œé›¶å»¶è¿Ÿ)
        prompt_text: è‡ªå®šä¹‰éŸ³è‰²çš„æç¤ºæ–‡æœ¬ (ä¸ prompt_wav é…åˆä½¿ç”¨)
        prompt_wav: è‡ªå®šä¹‰éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘ (ä¸ prompt_text é…åˆä½¿ç”¨)
        stream: æ˜¯å¦æµå¼è¾“å‡º
    """
    global cosyvoice, voice_cache
    logger.info(f"âš¡ å‚æ•°: voice_id{voice_id}(é›¶I/O/è®¡ç®—)")
    with inference_lock:
        try:
            # ç¡®å®šä½¿ç”¨å“ªä¸ªéŸ³è‰²
            spk_id = ""
            actual_prompt_text = prompt_text
            actual_prompt_wav = prompt_wav
            
            # ä¼˜å…ˆä½¿ç”¨ voice_id (é¢„ç¼“å­˜éŸ³è‰²)
            if voice_id and voice_id in voice_cache:
                spk_id = voice_id
                voice_info = voice_cache[voice_id]
                actual_prompt_text = voice_info["prompt_text"]
                actual_prompt_wav = voice_info["file"]
                logger.debug(f"âš¡ ä½¿ç”¨é¢„ç¼“å­˜éŸ³è‰²: {voice_id} (é›¶I/O/è®¡ç®—)")
            elif voice_id and voice_id not in voice_cache:
                logger.warning(f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤éŸ³è‰²")
                if default_voice_id in voice_cache:
                    spk_id = default_voice_id
                    voice_info = voice_cache[default_voice_id]
                    actual_prompt_text = voice_info["prompt_text"]
                    actual_prompt_wav = voice_info["file"]
            elif prompt_text and prompt_wav:
                # ä½¿ç”¨è‡ªå®šä¹‰éŸ³è‰² (æ— ç¼“å­˜ï¼Œéœ€å®æ—¶è®¡ç®—)
                logger.debug("ä½¿ç”¨è‡ªå®šä¹‰éŸ³è‰² (å®æ—¶è®¡ç®—ç‰¹å¾)")
            else:
                # ä½¿ç”¨é»˜è®¤éŸ³è‰²
                if default_voice_id in voice_cache:
                    spk_id = default_voice_id
                    voice_info = voice_cache[default_voice_id]
                    actual_prompt_text = voice_info["prompt_text"]
                    actual_prompt_wav = voice_info["file"]
                    logger.debug(f"âš¡ ä½¿ç”¨é»˜è®¤éŸ³è‰²: {default_voice_id}")
                
            for result in cosyvoice.inference_zero_shot(
                text, 
                actual_prompt_text, 
                actual_prompt_wav,
                stream=stream,
                zero_shot_spk_id=spk_id
            ):
                audio_tensor = result['tts_speech']
                
                # [GPU é‡é‡‡æ ·] å¦‚æœè¾“å‡ºé‡‡æ ·ç‡ä¸æ¨¡å‹åŸç”Ÿä¸åŒï¼Œè¿›è¡Œé‡é‡‡æ ·
                if output_sample_rate != cosyvoice.sample_rate:
                    audio_tensor = torchaudio.functional.resample(
                        audio_tensor, 
                        orig_freq=cosyvoice.sample_rate, 
                        new_freq=output_sample_rate
                    )
                
                # [GPU ç‰ˆæœ¬] PCM 16bit è½¬æ¢
                # 1. GPU è¿›è¡Œä¹˜æ³• (* 32768)
                # 2. GPU è¿›è¡Œç±»å‹è½¬æ¢ (float -> int16)
                # 3. ä¼ è¾“ int16 (2 bytes) åˆ° CPU
                yield (audio_tensor * 32768).to(torch.int16).cpu().numpy().tobytes()
        except Exception as e:
            logger.error(f"TTS ç”Ÿæˆå¤±è´¥: {e}")
            raise
        
@app.get("/tts/voices")
async def voices():
    return JSONResponse(VOICE_CONFIGS)


@app.get("/health")
async def health_check():
    mps_is_built = bool(getattr(getattr(torch, "backends", None), "mps", None) and torch.backends.mps.is_built())
    mps_is_available = bool(getattr(getattr(torch, "backends", None), "mps", None) and torch.backends.mps.is_available())
    mps_memory = {}
    if mps_is_available and hasattr(torch, "mps"):
        try:
            allocated = getattr(torch.mps, "current_allocated_memory", None)
            driver_allocated = getattr(torch.mps, "driver_allocated_memory", None)
            if callable(allocated):
                mps_memory["allocated_bytes"] = int(allocated())
            if callable(driver_allocated):
                mps_memory["driver_allocated_bytes"] = int(driver_allocated())
        except Exception:
            mps_memory = {}

    cuda_info = {"is_available": bool(torch.cuda.is_available())}
    if torch.cuda.is_available():
        cuda_info.update(
            {
                "device_count": int(torch.cuda.device_count()),
                "device_name": torch.cuda.get_device_name(0),
                "memory_allocated_bytes": int(torch.cuda.memory_allocated(0)),
                "memory_reserved_bytes": int(torch.cuda.memory_reserved(0)),
            }
        )

    model_device = None
    if cosyvoice is not None:
        if hasattr(cosyvoice, "model") and hasattr(cosyvoice.model, "device"):
            model_device = str(cosyvoice.model.device)
        else:
            try:
                model_device = str(next(cosyvoice.parameters()).device)
            except Exception:
                model_device = None

    return JSONResponse(
        {
            "status": "ok",
            "platform": sys.platform,
            "python_version": sys.version.split(" ")[0],
            "torch_version": torch.__version__,
            "model": "Fun-CosyVoice3-0.5B-2512",
            "model_sample_rate": cosyvoice.sample_rate if cosyvoice else None,
            "output_sample_rate": output_sample_rate,
            "model_device": model_device,
            "available_voices": list(voice_cache.keys()),
            "default_voice": default_voice_id,
            "accelerators": {
                "cuda": cuda_info,
                "mps": {
                    "is_built": mps_is_built,
                    "is_available": mps_is_available,
                    "memory": mps_memory,
                },
            },
        }
    )


@app.post("/tts/stream")
async def tts_stream(
    text: str = Form(..., description="è¦åˆæˆçš„æ–‡æœ¬"),
    voice_id: Optional[str] = Form(default=None, description="éŸ³è‰²ID (ä½¿ç”¨é¢„åŠ è½½çš„éŸ³è‰²ï¼Œé›¶å»¶è¿Ÿ)"),
    prompt_text: Optional[str] = Form(default=None, description="è‡ªå®šä¹‰éŸ³è‰²çš„æç¤ºæ–‡æœ¬"),
    prompt_wav: Optional[UploadFile] = File(default=None, description="è‡ªå®šä¹‰éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘")
):
    """
    æµå¼ TTS æ¥å£
    
    å‚æ•°ä¼˜å…ˆçº§:
    1. voice_id: ä½¿ç”¨é¢„åŠ è½½çš„éŸ³è‰² (æ¨èï¼Œé›¶å»¶è¿Ÿ)
    2. prompt_text + prompt_wav: è‡ªå®šä¹‰éŸ³è‰² (éœ€å®æ—¶è®¡ç®—ç‰¹å¾)
    3. éƒ½ä¸ä¼ : ä½¿ç”¨é»˜è®¤éŸ³è‰²
    
    è¿”å›: æµå¼ PCM éŸ³é¢‘æ•°æ® (é‡‡æ ·ç‡ç”± --output_sample_rate æ§åˆ¶, 16bit, Mono)
    """
    if not text or len(text.strip()) == 0:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
    
    # å¤„ç†è‡ªå®šä¹‰éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘
    prompt_wav_data = None
    if prompt_wav is not None:
        prompt_wav_data = prompt_wav.file
    
    # è®°å½•æ—¥å¿—
    if voice_id:
        logger.info(f"TTS è¯·æ±‚: text='{text[:50]}...', voice_id='{voice_id}'")
    elif prompt_text:
        logger.info(f"TTS è¯·æ±‚: text='{text[:50]}...', prompt_text='{prompt_text[:30]}...' (è‡ªå®šä¹‰éŸ³è‰²)")
    else:
        logger.info(f"TTS è¯·æ±‚: text='{text[:50]}...', voice_id='default'")
    
    start_time = time.time()
    
    def stream_generator():
        first_chunk = True
        total_bytes = 0
        for chunk in generate_audio_stream(
            text, 
            voice_id=voice_id,
            prompt_text=prompt_text, 
            prompt_wav=prompt_wav_data, 
            stream=True
        ):
            if first_chunk:
                logger.info(f"âš¡ é¦–å¸§å»¶è¿Ÿ: {(time.time() - start_time) * 1000:.0f}ms")
                first_chunk = False
            total_bytes += len(chunk)
            yield chunk
        logger.info(f"âœ… TTS å®Œæˆ: æ€»è€—æ—¶ {(time.time() - start_time) * 1000:.0f}ms, æ•°æ®é‡ {total_bytes / 1024:.1f}KB")
    
    return StreamingResponse(
        stream_generator(),
        media_type="application/octet-stream",
        headers={
            "X-Sample-Rate": str(output_sample_rate),
            "X-Channels": "1",
            "X-Bits": "16"
        }
    )

@app.post("/tts/instruct2")
async def instruct2(
    text: str = Form(..., description="è¦åˆæˆçš„æ–‡æœ¬"),
    voice_id: Optional[str] = Form(default=None, description="éŸ³è‰²ID (ä½¿ç”¨é¢„åŠ è½½çš„éŸ³è‰²ï¼Œé›¶å»¶è¿Ÿ)"),
    motionctrl: str = Form(..., description="æƒ…æ„Ÿæ§åˆ¶"),
): 
    if not voice_id:
        raise HTTPException(status_code=400, detail="voice_id ä¸èƒ½ä¸ºç©º")
    if voice_id not in voice_cache:
        raise HTTPException(status_code=404, detail=f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    fns = generate_instruct2(text,voice_id,motionctrl)
    
    return FileResponse(
        path=fns,
        filename=fns,  # å®¢æˆ·ç«¯ä¸‹è½½æ—¶æ˜¾ç¤ºçš„æ–‡ä»¶å
        media_type="audio/wav"  # å¯é€‰çš„ MIME ç±»å‹
    )

@app.post("/tts/zs_sync")
async def instruct2(
    text: str = Form(..., description="è¦åˆæˆçš„æ–‡æœ¬"),
    voice_id: Optional[str] = Form(default=None, description="éŸ³è‰²ID (ä½¿ç”¨é¢„åŠ è½½çš„éŸ³è‰²ï¼Œé›¶å»¶è¿Ÿ)"),
): 
    if not voice_id:
        raise HTTPException(status_code=400, detail="voice_id ä¸èƒ½ä¸ºç©º")
    if voice_id not in voice_cache:
        raise HTTPException(status_code=404, detail=f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    fns = generate_zs_sync(text,voice_id)
    logger.info(f"å‚æ•°: voice_id{voice_id} {text} ,æ–‡ä»¶è·¯å¾„: {fns}")
    
    return FileResponse(
        path=fns,
        filename=fns,  # å®¢æˆ·ç«¯ä¸‹è½½æ—¶æ˜¾ç¤ºçš„æ–‡ä»¶å
        media_type="audio/wav"  # å¯é€‰çš„ MIME ç±»å‹
    )    

@app.post("/tts/cross_lingual_sync")
async def cross_lingual_sync(
    text: str = Form(..., description="è¦åˆæˆçš„æ–‡æœ¬"),
    voice_id: Optional[str] = Form(default=None, description="éŸ³è‰²ID (ä½¿ç”¨é¢„åŠ è½½çš„éŸ³è‰²ï¼Œé›¶å»¶è¿Ÿ)"),
): 
    if not voice_id:
        raise HTTPException(status_code=400, detail="voice_id ä¸èƒ½ä¸ºç©º")
    if voice_id not in voice_cache:
        raise HTTPException(status_code=404, detail=f"éŸ³è‰² '{voice_id}' ä¸å­˜åœ¨")
    fns = generate_cross_lingual_sync(text,voice_id)
    logger.info(f"å‚æ•°: voice_id{voice_id} {text} ,æ–‡ä»¶è·¯å¾„: {fns}")
    return FileResponse(
        path=fns,
        filename=fns,  # å®¢æˆ·ç«¯ä¸‹è½½æ—¶æ˜¾ç¤ºçš„æ–‡ä»¶å
        media_type="audio/wav"  # å¯é€‰çš„ MIME ç±»å‹
    )    


@app.post("/tts/zero_shot")
async def tts_zero_shot(
    text: str = Form(..., description="è¦åˆæˆçš„æ–‡æœ¬"),
    prompt_text: str = Form(..., description="å‚è€ƒéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬"),
    prompt_wav: UploadFile = File(..., description="å‚è€ƒéŸ³é¢‘æ–‡ä»¶ (WAV, 16kHz)")
):
    """
    Zero-shot éŸ³è‰²å…‹éš†æ¥å£
    
    è¿”å›: æµå¼ PCM éŸ³é¢‘æ•°æ®
    """
    if not text or len(text.strip()) == 0:
        raise HTTPException(status_code=400, detail="æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
    if not prompt_text or len(prompt_text.strip()) == 0:
        raise HTTPException(status_code=400, detail="æç¤ºæ–‡æœ¬ä¸èƒ½ä¸ºç©º")
    
    try:
        # ç›´æ¥ä¼ å…¥æ–‡ä»¶å¯¹è±¡ (SpooledTemporaryFile)
        prompt_wav_data = prompt_wav.file
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"éŸ³é¢‘æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
    
    logger.info(f"Zero-shot TTS: text='{text[:50]}...', prompt='{prompt_text[:30]}...'")
    
    def stream_generator():
        for chunk in generate_audio_stream(text, prompt_text, prompt_wav_data, stream=True):
            yield chunk
    
    return StreamingResponse(
        stream_generator(),
        media_type="application/octet-stream",
        headers={
            "X-Sample-Rate": str(cosyvoice.sample_rate),
            "X-Channels": "1",
            "X-Bits": "16"
        }
    )


def load_model(model_dir: str, device: str = "mps", fp16: bool = False, use_vllm: bool = False):
    """åŠ è½½ CosyVoice æ¨¡å‹"""
    global cosyvoice, voice_cache
    from cosyvoice.cli.cosyvoice import AutoModel
    
    logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_dir}")
    logger.info(f"è®¾å¤‡: {device}, FP16: {fp16}, vLLMåŠ é€Ÿ: {use_vllm}")
    
    if use_vllm:
        try:
            import vllm
        except ImportError:
            logger.error("å¯ç”¨ vLLM å¤±è´¥: æœªæ‰¾åˆ° vllm åº“ã€‚è¯·å…ˆå®‰è£…: pip install vllm==0.9.0")
            sys.exit(1)
            
    start_time = time.time()
    try:
        # load_vllm å‚æ•°ä¼šè‡ªåŠ¨è§¦å‘ CosyVoice2/3 æ¨¡å‹çš„ vLLM åŠ è½½é€»è¾‘
        # æ³¨æ„: åº“å†…éƒ¨é»˜è®¤ gpu_memory_utilization=0.2 (é€‚é… 24G æ˜¾å­˜)
        cosyvoice = AutoModel(model_dir=model_dir, fp16=fp16, load_vllm=use_vllm)
    except TypeError as e:
        if "load_vllm" in str(e):
             logger.error("å½“å‰ CosyVoice ç‰ˆæœ¬ä¼¼ä¹ä¸æ”¯æŒ vLLMï¼Œè¯·ç¡®ä¿ä½¿ç”¨æœ€æ–°ä»£ç ")
        raise e
        
    logger.info(f"æ¨¡å‹åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.1f}s")
    logger.info(f"æ¨¡å‹é‡‡æ ·ç‡: {cosyvoice.sample_rate}Hz, è¾“å‡ºé‡‡æ ·ç‡: {output_sample_rate}Hz")
    
    # ========== åŠ è½½å¤šéŸ³è‰²é…ç½® ==========
    asset_dir = os.path.join(SCRIPT_DIR, "../asset")
    official_asset_dir = os.path.join(SCRIPT_DIR, "official", "asset")
    
    logger.info(f"âš¡ æ­£åœ¨åŠ è½½ {len(VOICE_CONFIGS)} ä¸ªéŸ³è‰²é…ç½®...")
    
    for voice_config in VOICE_CONFIGS:
        voice_id = voice_config["id"]
        voice_file = voice_config["file"]
        prompt_text = voice_config["prompt_text"]
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        voice_path = os.path.join(asset_dir, voice_file)
        if not os.path.exists(voice_path):
            # å°è¯•ä»å®˜æ–¹ç›®å½•æŸ¥æ‰¾
            voice_path = os.path.join(official_asset_dir, voice_file)
        
        if not os.path.exists(voice_path):
            logger.warning(f"âŒ éŸ³è‰² '{voice_id}' çš„æ–‡ä»¶æœªæ‰¾åˆ°: {voice_file}")
            continue
        
        try:
            # ç¼“å­˜éŸ³è‰²ç‰¹å¾
            cosyvoice.add_zero_shot_spk(prompt_text, voice_path, voice_id)
            
            # ä¿å­˜åˆ° voice_cache
            voice_cache[voice_id] = {
                "file": voice_path,
                "prompt_text": prompt_text
            }
            logger.info(f"âœ… éŸ³è‰² '{voice_id}' åŠ è½½æˆåŠŸ: {voice_file}")
        except Exception as e:
            logger.warning(f"âŒ éŸ³è‰² '{voice_id}' åŠ è½½å¤±è´¥: {e}")
    
    logger.info(f"âš¡ éŸ³è‰²åŠ è½½å®Œæˆï¼Œå…± {len(voice_cache)} ä¸ªå¯ç”¨éŸ³è‰²: {list(voice_cache.keys())}")
    
    # é¢„çƒ­æ¨ç† (ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨éŸ³è‰²)
    first_voice_path = None
    if voice_cache:
        first_voice_id = list(voice_cache.keys())[0]
        first_voice_path = voice_cache[first_voice_id]["file"]
    warmup_model(first_voice_path, first_voice_id if voice_cache else None)
    
    return cosyvoice


def warmup_model(prompt_wav_path: str = None, voice_id: str = None):
    """é¢„çƒ­æ¨¡å‹ï¼Œå‡å°‘é¦–æ¬¡è¯·æ±‚å»¶è¿Ÿ"""
    global cosyvoice
    
    if cosyvoice is None:
        return
    
    logger.info("ğŸ”¥ æ­£åœ¨é¢„çƒ­æ¨¡å‹...")
    start_time = time.time()
    
    warmup_text = "é¢„çƒ­æµ‹è¯•"
    warmup_prompt_text = "é¢„çƒ­"
    
    # å¦‚æœæœ‰å‚è€ƒéŸ³é¢‘ï¼Œä½¿ç”¨ zero-shot é¢„çƒ­
    if prompt_wav_path and os.path.exists(prompt_wav_path):
        try:
            # ä½¿ç”¨æŒ‡å®šçš„ voice_id è¿›è¡Œé¢„çƒ­
            spk_id = voice_id if voice_id else "default"
            for _ in cosyvoice.inference_zero_shot(
                warmup_text, 
                warmup_prompt_text, 
                prompt_wav_path,
                stream=False,
                zero_shot_spk_id=spk_id
            ):
                pass
            logger.info(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.1f}s")
        except Exception as e:
            logger.warning(f"é¢„çƒ­å¤±è´¥ (ä¸å½±å“æ­£å¸¸ä½¿ç”¨): {e}")
    else:
        logger.info("â­ è·³è¿‡é¢„çƒ­ (æ— å‚è€ƒéŸ³é¢‘)")


def main():
    global output_sample_rate
    
    parser = argparse.ArgumentParser(description="CosyVoice TTS Server")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="ç›‘å¬åœ°å€")
    parser.add_argument("--port", type=int, default=9662, help="ç›‘å¬ç«¯å£")
    parser.add_argument(
        "--model_dir", 
        type=str, 
        default="../pretrained_models/Fun-CosyVoice3-0.5B",
        help="æ¨¡å‹ç›®å½•è·¯å¾„"
    )
    parser.add_argument("--device", type=str, default="cuda", help="è¿è¡Œè®¾å¤‡: cuda æˆ– cpu")
    parser.add_argument("--fp16", action="store_true", help="ä½¿ç”¨ FP16 æ¨ç† (èŠ‚çœæ˜¾å­˜)")
    parser.add_argument("--use_vllm", action="store_true", help="[ä¼˜åŒ–] ä½¿ç”¨ vLLM åŠ é€Ÿæ¨ç† (éœ€ pip install vllm)")
    parser.add_argument(
        "--output_sample_rate", 
        type=int, 
        default=16000, 
        choices=[16000, 24000],
        help="è¾“å‡ºé‡‡æ ·ç‡: 16000 (å…¼å®¹å°æ™ºå¹³å°) æˆ– 24000 (åŸç”Ÿé«˜è´¨é‡)"
    )
    args = parser.parse_args()
    
    # è®¾ç½®è¾“å‡ºé‡‡æ ·ç‡
    output_sample_rate = args.output_sample_rate
    
    # å¤„ç†ç›¸å¯¹è·¯å¾„
    if not os.path.isabs(args.model_dir):
        args.model_dir = os.path.join(SCRIPT_DIR, args.model_dir)
    
    # åŠ è½½æ¨¡å‹
    load_model(args.model_dir, args.device, args.fp16, args.use_vllm)
    
    # å¯åŠ¨æœåŠ¡
    logger.info(f"æœåŠ¡å·²å¯åŠ¨: http://{args.host}:{args.port}")
    logger.info(f"å¥åº·æ£€æŸ¥: http://{args.host}:{args.port}/health")
    logger.info(f"TTS æ¥å£: POST http://{args.host}:{args.port}/tts/stream")
    logger.info(f"ğŸ“¢ è¾“å‡ºé‡‡æ ·ç‡: {output_sample_rate}Hz (æ¨¡å‹åŸç”Ÿ: 24000Hz)")
    
    uvicorn.run(app, host=args.host, port=args.port)


if __name__ == "__main__":
    main()
